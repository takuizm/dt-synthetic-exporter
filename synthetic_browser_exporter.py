#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€çµ‚ç‰ˆBrowserç›£è¦–çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ v2 (ä¿®æ­£ç‰ˆ)
ä½œæˆæ—¥: 2025-01-26

67å€‹ã®å‹•ä½œç¢ºèªæ¸ˆã¿ãƒ¡ãƒˆãƒªã‚¯ã‚¹ + ãƒ‡ãƒ¼ã‚¿æ§‹é€ ä¿®æ­£å¯¾å¿œ
"""

import os
import json
import logging
import requests
import pandas as pd
import argparse
import pytz
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any


def setup_logging():
    """ãƒ­ã‚°è¨­å®š"""
    log_dir = Path('.logs')
    log_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = log_dir / f'ultimate_browser_export_v2_{timestamp}.log'
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


class UltimateBrowserExporterV2:
    """æœ€çµ‚ç‰ˆBrowserç›£è¦–çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ v2"""
    
    # å‹•ä½œç¢ºèªæ¸ˆã¿67å€‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    WORKING_METRICS = [
        'builtin:synthetic.browser.actionDuration.load',
        'builtin:synthetic.browser.actionDuration.load.geo',
        'builtin:synthetic.browser.availability',
        'builtin:synthetic.browser.availability.location.total',
        'builtin:synthetic.browser.availability.location.totalWoMaintenanceWindow',
        'builtin:synthetic.browser.cumulativeLayoutShift.load',
        'builtin:synthetic.browser.cumulativeLayoutShift.load.geo',
        'builtin:synthetic.browser.domInteractive.load',
        'builtin:synthetic.browser.domInteractive.load.geo',
        'builtin:synthetic.browser.event.actionDuration.load',
        'builtin:synthetic.browser.event.actionDuration.load.geo',
        'builtin:synthetic.browser.event.cumulativeLayoutShift.load',
        'builtin:synthetic.browser.event.cumulativeLayoutShift.load.geo',
        'builtin:synthetic.browser.event.domInteractive.load',
        'builtin:synthetic.browser.event.domInteractive.load.geo',
        'builtin:synthetic.browser.event.failure',
        'builtin:synthetic.browser.event.failure.geo',
        'builtin:synthetic.browser.event.firstByte.load',
        'builtin:synthetic.browser.event.firstByte.load.geo',
        'builtin:synthetic.browser.event.largestContentfulPaint.load',
        'builtin:synthetic.browser.event.largestContentfulPaint.load.geo',
        'builtin:synthetic.browser.event.loadEventEnd.load',
        'builtin:synthetic.browser.event.loadEventEnd.load.geo',
        'builtin:synthetic.browser.event.loadEventStart.load',
        'builtin:synthetic.browser.event.loadEventStart.load.geo',
        'builtin:synthetic.browser.event.networkContribution.load',
        'builtin:synthetic.browser.event.networkContribution.load.geo',
        'builtin:synthetic.browser.event.responseEnd.load',
        'builtin:synthetic.browser.event.responseEnd.load.geo',
        'builtin:synthetic.browser.event.serverContribution.load',
        'builtin:synthetic.browser.event.serverContribution.load.geo',
        'builtin:synthetic.browser.event.speedIndex.load',
        'builtin:synthetic.browser.event.speedIndex.load.geo',
        'builtin:synthetic.browser.event.success',
        'builtin:synthetic.browser.event.success.geo',
        'builtin:synthetic.browser.event.total',
        'builtin:synthetic.browser.event.total.geo',
        'builtin:synthetic.browser.event.totalDuration',
        'builtin:synthetic.browser.event.totalDuration.geo',
        'builtin:synthetic.browser.event.visuallyComplete.load',
        'builtin:synthetic.browser.event.visuallyComplete.load.geo',
        'builtin:synthetic.browser.failure',
        'builtin:synthetic.browser.failure.geo',
        'builtin:synthetic.browser.firstByte.load',
        'builtin:synthetic.browser.firstByte.load.geo',
        'builtin:synthetic.browser.largestContentfulPaint.load',
        'builtin:synthetic.browser.largestContentfulPaint.load.geo',
        'builtin:synthetic.browser.loadEventEnd.load',
        'builtin:synthetic.browser.loadEventEnd.load.geo',
        'builtin:synthetic.browser.loadEventStart.load',
        'builtin:synthetic.browser.loadEventStart.load.geo',
        'builtin:synthetic.browser.networkContribution.load',
        'builtin:synthetic.browser.networkContribution.load.geo',
        'builtin:synthetic.browser.responseEnd.load',
        'builtin:synthetic.browser.responseEnd.load.geo',
        'builtin:synthetic.browser.serverContribution.load',
        'builtin:synthetic.browser.serverContribution.load.geo',
        'builtin:synthetic.browser.speedIndex.load',
        'builtin:synthetic.browser.speedIndex.load.geo',
        'builtin:synthetic.browser.success',
        'builtin:synthetic.browser.success.geo',
        'builtin:synthetic.browser.total',
        'builtin:synthetic.browser.total.geo',
        'builtin:synthetic.browser.totalDuration',
        'builtin:synthetic.browser.totalDuration.geo',
        'builtin:synthetic.browser.visuallyComplete.load',
        'builtin:synthetic.browser.visuallyComplete.load.geo'
    ]
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©³ç´°æƒ…å ±ï¼ˆWeb Vitalsæº–æ‹ ï¼‰
    METRIC_INFO = {
        # Core Web Vitals
        'builtin:synthetic.browser.largestContentfulPaint.load': {
            'display_name': 'Largest Contentful Paint (LCP)',
            'description': 'æœ€å¤§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æç”»æ™‚é–“',
            'unit': 'ms',
            'category': 'Core Web Vitals',
            'good_threshold': 2500,
            'warning_threshold': 4000
        },
        'builtin:synthetic.browser.cumulativeLayoutShift.load': {
            'display_name': 'Cumulative Layout Shift (CLS)',
            'description': 'ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚·ãƒ•ãƒˆã®ç´¯ç©ã‚¹ã‚³ã‚¢',
            'unit': 'score',
            'category': 'Core Web Vitals',
            'good_threshold': 0.1,
            'warning_threshold': 0.25
        },
        
        # Performance Metrics
        'builtin:synthetic.browser.actionDuration.load': {
            'display_name': 'ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰æ™‚é–“',
            'description': 'ãƒšãƒ¼ã‚¸ã®å®Œå…¨ãƒ­ãƒ¼ãƒ‰ã«ã‹ã‹ã‚‹æ™‚é–“',
            'unit': 'ms',
            'category': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹',
            'good_threshold': 3000,
            'warning_threshold': 5000
        },
        'builtin:synthetic.browser.speedIndex.load': {
            'display_name': 'Speed Index',
            'description': 'ãƒšãƒ¼ã‚¸ã®è¦–è¦šçš„ãªèª­ã¿è¾¼ã¿é€Ÿåº¦',
            'unit': 'ms',
            'category': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹',
            'good_threshold': 3400,
            'warning_threshold': 5800
        },
        'builtin:synthetic.browser.visuallyComplete.load': {
            'display_name': 'Visually Complete',
            'description': 'ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆã®å®Œå…¨ãªæç”»æ™‚é–“',
            'unit': 'ms',
            'category': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹',
            'good_threshold': 4000,
            'warning_threshold': 6000
        },
        'builtin:synthetic.browser.firstByte.load': {
            'display_name': 'Time to First Byte (TTFB)',
            'description': 'æœ€åˆã®ãƒã‚¤ãƒˆå—ä¿¡æ™‚é–“',
            'unit': 'ms',
            'category': 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯',
            'good_threshold': 800,
            'warning_threshold': 1800
        },
        'builtin:synthetic.browser.domInteractive.load': {
            'display_name': 'DOM Interactive',
            'description': 'DOMãŒæ“ä½œå¯èƒ½ã«ãªã‚‹ã¾ã§ã®æ™‚é–“',
            'unit': 'ms',
            'category': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹',
            'good_threshold': 2000,
            'warning_threshold': 4000
        },
        
        # Availability
        'builtin:synthetic.browser.availability': {
            'display_name': 'å¯ç”¨æ€§',
            'description': 'ç›£è¦–ã®æˆåŠŸç‡',
            'unit': '%',
            'category': 'å¯ç”¨æ€§',
            'good_threshold': 99.0,
            'warning_threshold': 95.0
        }
    }
    
    def __init__(self, api_token: str, env_url: str):
        """åˆæœŸåŒ–"""
        self.api_token = api_token
        self.env_url = env_url.rstrip('/')
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Api-Token {api_token}',
            'Content-Type': 'application/json'
        })
        self.logger = logging.getLogger(__name__)
    
    def load_env_from_file(self):
        """ç’°å¢ƒå¤‰æ•°ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        env_file = Path('.env')
        if env_file.exists():
            with open(env_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key] = value
    
    def get_browser_monitors(self, tags: Optional[List[str]] = None) -> List[Dict]:
        """Browserç›£è¦–ä¸€è¦§ã‚’å–å¾—"""
        url = f"{self.env_url}/api/v1/synthetic/monitors"
        params = {}
        
        if tags:
            params['tag'] = tags
        
        self.logger.info(f"Browserç›£è¦–ä¸€è¦§å–å¾—ä¸­... ãƒ•ã‚£ãƒ«ã‚¿: {params}")
        
        try:
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            monitors = response.json().get('monitors', [])
            # Browserç›£è¦–ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
            browser_monitors = [m for m in monitors if m.get('type') == 'BROWSER']
            
            self.logger.info(f"å–å¾—ã—ãŸBrowserç›£è¦–æ•°: {len(browser_monitors)}")
            return browser_monitors
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"ç›£è¦–ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def get_monitor_details(self, monitor_id: str) -> Optional[Dict]:
        """ç›£è¦–ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        url = f"{self.env_url}/api/v1/synthetic/monitors/{monitor_id}"
        
        try:
            response = self.session.get(url)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            self.logger.error(f"ç›£è¦–è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ ({monitor_id}): {e}")
            return None
    
    def get_metrics_data(self, 
                        metric_selector: str,
                        entity_selector: str,
                        from_time: str,
                        to_time: str,
                        resolution: str = "1h") -> Optional[Dict]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        url = f"{self.env_url}/api/v2/metrics/query"
        
        params = {
            'metricSelector': metric_selector,
            'entitySelector': entity_selector,
            'from': from_time,
            'to': to_time,
            'resolution': resolution
        }
        
        try:
            response = self.session.get(url, params=params)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            self.logger.debug(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼ ({metric_selector}): {e}")
            return None
    
    def collect_comprehensive_data(self,
                                 tags: Optional[List[str]] = None,
                                 hours: int = 24,
                                 resolution: str = "1h",
                                 include_geo: bool = True) -> pd.DataFrame:
        """åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿åé›†"""
        
        # æ™‚é–“ç¯„å›²è¨­å®šï¼ˆJSTåŸºæº–ã€ã‚ˆã‚Šæ­£ç¢ºãª24æ™‚é–“ï¼‰
        import pytz
        jst = pytz.timezone('Asia/Tokyo')
        
        # ç¾åœ¨ã®JSTæ™‚åˆ»ã‚’å–å¾—
        now_jst = datetime.now(jst)
        from_jst = now_jst - timedelta(hours=hours)
        
        # è§£åƒåº¦ã«å¿œã˜ã¦æ™‚åˆ»ã‚’èª¿æ•´ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®å¢ƒç•Œã«åˆã‚ã›ã‚‹ï¼‰
        # raw_timeã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿èª¿æ•´
        if not getattr(self, 'raw_time', False):
            if resolution == "5m":
                # 5åˆ†å˜ä½ã«èª¿æ•´
                from_jst = from_jst.replace(minute=(from_jst.minute // 5) * 5, second=0, microsecond=0)
                now_jst = now_jst.replace(minute=(now_jst.minute // 5) * 5, second=0, microsecond=0)
            elif resolution == "15m":
                # 15åˆ†å˜ä½ã«èª¿æ•´
                from_jst = from_jst.replace(minute=(from_jst.minute // 15) * 15, second=0, microsecond=0)
                now_jst = now_jst.replace(minute=(now_jst.minute // 15) * 15, second=0, microsecond=0)
            elif resolution == "1h":
                # 1æ™‚é–“å˜ä½ã«èª¿æ•´
                from_jst = from_jst.replace(minute=0, second=0, microsecond=0)
                now_jst = now_jst.replace(minute=0, second=0, microsecond=0)
        
        # UTCã«å¤‰æ›ã—ã¦APIç”¨æ–‡å­—åˆ—ä½œæˆ
        to_time_utc = now_jst.astimezone(pytz.UTC)
        from_time_utc = from_jst.astimezone(pytz.UTC)
        
        from_str = from_time_utc.strftime('%Y-%m-%dT%H:%M:%S.000Z')
        to_str = to_time_utc.strftime('%Y-%m-%dT%H:%M:%S.000Z')
        
        self.logger.info(f"JSTæ™‚åˆ»ç¯„å›²: {from_jst.strftime('%Y-%m-%d %H:%M:%S JST')} ï½ {now_jst.strftime('%Y-%m-%d %H:%M:%S JST')}")
        self.logger.info(f"APIé€ä¿¡æ™‚åˆ»ç¯„å›²: {from_str} ï½ {to_str}")
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿è§£åƒåº¦: {resolution}")
        
        # Browserç›£è¦–ä¸€è¦§å–å¾—
        monitors = self.get_browser_monitors(tags)
        if not monitors:
            self.logger.warning("Browserç›£è¦–ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return pd.DataFrame()
        
        # ä½¿ç”¨ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹é¸æŠ
        metrics_to_use = self.WORKING_METRICS
        if not include_geo:
            # .geoã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é™¤å¤–
            metrics_to_use = [m for m in self.WORKING_METRICS if not m.endswith('.geo')]
        
        self.logger.info(f"ä½¿ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°: {len(metrics_to_use)}")
        
        all_data = []
        
        for monitor in monitors:
            monitor_id = monitor['entityId']
            monitor_name = monitor['name']
            monitor_type = monitor.get('type', 'BROWSER')
            monitor_url = monitor.get('script', {}).get('configuration', {}).get('url', 'N/A')
            
            self.logger.info(f"å‡¦ç†ä¸­: {monitor_name} ({monitor_id})")
            
            # ç›£è¦–è©³ç´°å–å¾—
            details = self.get_monitor_details(monitor_id)
            
            # ã‚¿ã‚°æƒ…å ±å–å¾—
            tags_dict = {}
            if details and 'tags' in details:
                for tag in details['tags']:
                    if isinstance(tag, dict) and 'key' in tag and 'value' in tag:
                        tags_dict[tag['key']] = tag['value']
            
            # ç›£è¦–è¨­å®šæƒ…å ±
            monitor_info = {
                'frequency': details.get('frequencyMin', 'N/A') if details else 'N/A',
                'locations': len(details.get('locations', [])) if details else 0,
                'enabled': details.get('enabled', False) if details else False
            }
            
            # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
            for metric in metrics_to_use:
                self.logger.debug(f"  ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—: {metric}")
                
                entity_selector = f'type(SYNTHETIC_TEST),entityId("{monitor_id}")'
                metrics_data = self.get_metrics_data(
                    metric, entity_selector, from_str, to_str, resolution
                )
                
                if metrics_data and 'result' in metrics_data:
                    for result in metrics_data['result']:
                        metric_id = result.get('metricId', metric)
                        
                        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œ
                        if 'data' in result:
                            # v5æ§‹é€ ï¼ˆdataé…åˆ—ï¼‰
                            for data_item in result['data']:
                                if 'timestamps' in data_item and 'values' in data_item:
                                    timestamps = data_item['timestamps']
                                    values = data_item['values']
                                    dimension_map = data_item.get('dimensionMap', {})
                                    
                                    # timestampsã¨valuesã®é•·ã•ãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
                                    if len(timestamps) == len(values):
                                        for timestamp, value in zip(timestamps, values):
                                            # Noneã§ãªã„å€¤ã®ã¿ã‚’å‡¦ç†
                                            if value is not None:
                                                all_data.append(self._create_record(
                                                    timestamp, value, metric_id, monitor_id, monitor_name,
                                                    monitor_type, monitor_url, monitor_info, tags_dict,
                                                    dimension_map, resolution
                                                ))
                        
                        # æ—§æ§‹é€ ã«ã‚‚å¯¾å¿œï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
                        elif 'timestamps' in result and 'values' in result:
                            # v4æ§‹é€ ï¼ˆtimestamps/valuesï¼‰
                            timestamps = result['timestamps']
                            values = result['values']
                            dimension_map = result.get('dimensionMap', {})
                            
                            # timestampsã¨valuesã®é•·ã•ãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
                            if len(timestamps) == len(values):
                                for timestamp, value in zip(timestamps, values):
                                    # Noneã§ãªã„å€¤ã®ã¿ã‚’å‡¦ç†
                                    if value is not None:
                                        all_data.append(self._create_record(
                                            timestamp, value, metric_id, monitor_id, monitor_name,
                                            monitor_type, monitor_url, monitor_info, tags_dict,
                                            dimension_map, resolution
                                        ))
        
        if all_data:
            df = pd.DataFrame(all_data)
            self.logger.info(f"åé›†å®Œäº†: {len(df)} ãƒ¬ã‚³ãƒ¼ãƒ‰")
            return df
        else:
            self.logger.warning("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return pd.DataFrame()
    
    def _create_record(self, timestamp, value, metric_id, monitor_id, monitor_name,
                      monitor_type, monitor_url, monitor_info, tags_dict,
                      dimension_map, resolution) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ"""
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©³ç´°æƒ…å ±
        metric_info = self.METRIC_INFO.get(metric_id, {
            'display_name': metric_id.split('.')[-1],
            'description': metric_id,
            'unit': 'unknown',
            'category': 'ãã®ä»–'
        })
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        performance_status = self._evaluate_performance(
            metric_id, value, metric_info
        )
        
        # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’è¨˜éŒ²ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’JSTã«å¤‰æ›ï¼‰
        jst = pytz.timezone('Asia/Tokyo')
        utc_timestamp = pd.to_datetime(timestamp, unit='ms', utc=True)
        jst_timestamp = utc_timestamp.tz_convert(jst)
        
        record = {
            'timestamp': jst_timestamp.strftime('%Y-%m-%d %H:%M:%S JST'),
            'monitor_id': monitor_id,
            'monitor_name': monitor_name,
            'monitor_type': monitor_type,
            'monitor_url': monitor_url,
            'monitor_frequency': monitor_info['frequency'],
            'monitor_locations': monitor_info['locations'],
            'monitor_enabled': monitor_info['enabled'],
            'metric_name': metric_id,
            'metric_display_name': metric_info.get('display_name', metric_id),
            'metric_category': metric_info.get('category', 'ãã®ä»–'),
            'value': value,
            'unit': metric_info.get('unit', 'unknown'),
            'performance_status': performance_status,
            'resolution': resolution
        }
        
        # ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¿½åŠ 
        for dim_key, dim_value in dimension_map.items():
            record[f'dimension_{dim_key}'] = dim_value
        
        # ã‚¿ã‚°æƒ…å ±ã‚’è¿½åŠ 
        for tag_key, tag_value in tags_dict.items():
            record[f'tag_{tag_key}'] = tag_value
        
        return record
    
    def _evaluate_performance(self, metric_id: str, value: float, metric_info: Dict) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡"""
        good_threshold = metric_info.get('good_threshold')
        warning_threshold = metric_info.get('warning_threshold')
        
        if good_threshold is None or warning_threshold is None:
            return 'Unknown'
        
        # å¯ç”¨æ€§ç³»ã¯é«˜ã„æ–¹ãŒè‰¯ã„
        if 'availability' in metric_id:
            if value >= good_threshold:
                return 'Good'
            elif value >= warning_threshold:
                return 'Warning'
            else:
                return 'Critical'
        # CLSã¯ä½ã„æ–¹ãŒè‰¯ã„ï¼ˆã‚¹ã‚³ã‚¢ï¼‰
        elif 'cumulativeLayoutShift' in metric_id:
            if value <= good_threshold:
                return 'Good'
            elif value <= warning_threshold:
                return 'Warning'
            else:
                return 'Critical'
        # ãã®ä»–ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ç³»ã¯ä½ã„æ–¹ãŒè‰¯ã„
        else:
            if value <= good_threshold:
                return 'Good'
            elif value <= warning_threshold:
                return 'Warning'
            else:
                return 'Critical'
    
    def generate_comprehensive_summary(self, df: pd.DataFrame) -> Dict:
        """åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if df.empty:
            return {}
        
        summary = {
            'report_generated': datetime.now().isoformat(),
            'data_period': {
                'start': str(df['timestamp'].min()),
                'end': str(df['timestamp'].max()),
                'total_records': len(df)
            },
            'monitors': {
                'total_count': df['monitor_name'].nunique(),
                'monitor_list': df['monitor_name'].unique().tolist()
            },
            'metrics': {
                'total_records': len(df),
                'unique_metrics': df['metric_name'].nunique(),
                'metrics_collected': df['metric_name'].unique().tolist()
            },
            'performance_analysis': {},
            'web_vitals_analysis': {},
            'category_analysis': {}
        }
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        for category in df['metric_category'].unique():
            category_data = df[df['metric_category'] == category]
            summary['category_analysis'][category] = {
                'total_measurements': len(category_data),
                'unique_metrics': category_data['metric_name'].nunique(),
                'monitors_measured': category_data['monitor_name'].nunique()
            }
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        for metric in df['metric_name'].unique():
            metric_data = df[df['metric_name'] == metric]
            
            performance_counts = metric_data['performance_status'].value_counts().to_dict()
            
            summary['performance_analysis'][metric] = {
                'total_measurements': len(metric_data),
                'average_value': metric_data['value'].mean(),
                'median_value': metric_data['value'].median(),
                'min_value': metric_data['value'].min(),
                'max_value': metric_data['value'].max(),
                'std_value': metric_data['value'].std(),
                'performance_distribution': performance_counts,
                'monitors_measured': metric_data['monitor_name'].nunique()
            }
        
        # Core Web Vitalsç‰¹åˆ¥åˆ†æ
        web_vitals_metrics = [
            'builtin:synthetic.browser.largestContentfulPaint.load',
            'builtin:synthetic.browser.cumulativeLayoutShift.load'
        ]
        
        for metric in web_vitals_metrics:
            if metric in df['metric_name'].values:
                metric_data = df[df['metric_name'] == metric]
                good_count = len(metric_data[metric_data['performance_status'] == 'Good'])
                total_count = len(metric_data)
                
                summary['web_vitals_analysis'][metric] = {
                    'good_percentage': (good_count / total_count * 100) if total_count > 0 else 0,
                    'total_measurements': total_count,
                    'good_measurements': good_count
                }
        
        return summary
    
    def export_to_csv(self, df: pd.DataFrame, output_file: Optional[str] = None) -> str:
        """DataFrameã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›"""
        # output/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs('output', exist_ok=True)
        
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'output/ultimate_browser_results_v2_{timestamp}.csv'
        elif not output_file.startswith('output/'):
            output_file = f'output/{output_file}'
        
        df.to_csv(output_file, index=False, encoding='utf-8')
        self.logger.info(f"CSVå‡ºåŠ›å®Œäº†: {output_file}")
        return output_file
    
    def export_summary_report(self, summary: Dict, output_file: Optional[str] = None) -> str:
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONã§å‡ºåŠ›"""
        # output/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs('output', exist_ok=True)
        
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'output/ultimate_browser_summary_v2_{timestamp}.json'
        elif not output_file.startswith('output/'):
            output_file = f'output/{output_file}'
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å®Œäº†: {output_file}")
        return output_file

    def generate_monitor_analysis(self, df: pd.DataFrame) -> Dict:
        """ç›£è¦–å¯¾è±¡åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆæ–°æ©Ÿèƒ½ï¼‰"""
        if df.empty:
            return {}
        
        analysis = {
            'report_generated': datetime.now().isoformat(),
            'analysis_type': 'monitor_focused',
            'data_period': {
                'start': str(df['timestamp'].min()),
                'end': str(df['timestamp'].max()),
                'total_records': len(df)
            },
            'monitor_analysis': {}
        }
        
        # ç›£è¦–å¯¾è±¡åˆ¥ã®åˆ†æ
        for monitor_name in df['monitor_name'].unique():
            monitor_data = df[df['monitor_name'] == monitor_name]
            
            monitor_analysis = {
                'monitor_info': {
                    'monitor_name': monitor_name,
                    'total_measurements': len(monitor_data),
                    'unique_metrics': monitor_data['metric_name'].nunique(),
                    'data_period': {
                        'start': str(monitor_data['timestamp'].min()),
                        'end': str(monitor_data['timestamp'].max())
                    }
                },
                'metrics_analysis': {}
            }
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ¥çµ±è¨ˆ
            for metric_name in monitor_data['metric_name'].unique():
                metric_data = monitor_data[monitor_data['metric_name'] == metric_name]
                
                if len(metric_data) > 0:
                    monitor_analysis['metrics_analysis'][metric_name] = {
                        'total_measurements': len(metric_data),
                        'min_value': float(metric_data['value'].min()),
                        'max_value': float(metric_data['value'].max()),
                        'average_value': float(metric_data['value'].mean()),
                        'median_value': float(metric_data['value'].median()),
                        'std_value': float(metric_data['value'].std()) if len(metric_data) > 1 else 0.0,
                        'performance_distribution': metric_data['performance_status'].value_counts().to_dict()
                    }
            
            analysis['monitor_analysis'][monitor_name] = monitor_analysis
        
        return analysis

    def export_monitor_analysis_csv(self, df: pd.DataFrame, output_file: Optional[str] = None) -> str:
        """ç›£è¦–å¯¾è±¡åˆ¥åˆ†æã‚’CSVã§å‡ºåŠ›ï¼ˆæ–°æ©Ÿèƒ½ï¼‰"""
        if df.empty:
            self.logger.warning("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ç›£è¦–å¯¾è±¡åˆ¥åˆ†æCSVã‚’å‡ºåŠ›ã§ãã¾ã›ã‚“")
            return ""
        
        # output/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs('output', exist_ok=True)
        
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'output/monitor_analysis_{timestamp}.csv'
        elif not output_file.startswith('output/'):
            output_file = f'output/{output_file}'
        
        # ç›£è¦–å¯¾è±¡åˆ¥åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        analysis_rows = []
        
        for monitor_name in df['monitor_name'].unique():
            monitor_data = df[df['monitor_name'] == monitor_name]
            
            for metric_name in monitor_data['metric_name'].unique():
                metric_data = monitor_data[monitor_data['metric_name'] == metric_name]
                
                if len(metric_data) > 0:
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’çŸ­ç¸®ï¼ˆè¡¨ç¤ºç”¨ï¼‰
                    short_metric = metric_name.replace('builtin:synthetic.browser.', '').replace('.load', '')
                    
                    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†å¸ƒã‚’è¨ˆç®—
                    perf_dist = metric_data['performance_status'].value_counts()
                    good_count = perf_dist.get('Good', 0)
                    warning_count = perf_dist.get('Warning', 0)
                    critical_count = perf_dist.get('Critical', 0)
                    unknown_count = perf_dist.get('Unknown', 0)
                    
                    # ä¸»è¦ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ±ºå®š
                    total_graded = good_count + warning_count + critical_count
                    if total_graded > 0:
                        if good_count / total_graded >= 0.8:
                            overall_grade = 'Good'
                        elif critical_count / total_graded >= 0.1:
                            overall_grade = 'Critical'
                        else:
                            overall_grade = 'Warning'
                    else:
                        overall_grade = 'Unknown'
                    
                    analysis_rows.append({
                        'Monitor': monitor_name,
                        'Metric': short_metric,
                        'Min': round(metric_data['value'].min(), 2),
                        'Max': round(metric_data['value'].max(), 2),
                        'Average': round(metric_data['value'].mean(), 2),
                        'Median': round(metric_data['value'].median(), 2),
                        'StdDev': round(metric_data['value'].std(), 2) if len(metric_data) > 1 else 0.0,
                        'Measurements': len(metric_data),
                        'Good': good_count,
                        'Warning': warning_count,
                        'Critical': critical_count,
                        'Unknown': unknown_count,
                        'Performance_Grade': overall_grade
                    })
        
        # DataFrameã«å¤‰æ›ã—ã¦CSVå‡ºåŠ›
        analysis_df = pd.DataFrame(analysis_rows)
        analysis_df.to_csv(output_file, index=False, encoding='utf-8')
        
        self.logger.info(f"ç›£è¦–å¯¾è±¡åˆ¥åˆ†æCSVå‡ºåŠ›å®Œäº†: {output_file}")
        return output_file

    def export_monitor_summary_csv(self, df: pd.DataFrame, output_file: Optional[str] = None) -> str:
        """ç›£è¦–å¯¾è±¡åˆ¥ã‚µãƒãƒªãƒ¼ã‚’CSVã§å‡ºåŠ›ï¼ˆæ–°æ©Ÿèƒ½ï¼‰"""
        if df.empty:
            self.logger.warning("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ç›£è¦–å¯¾è±¡åˆ¥ã‚µãƒãƒªãƒ¼CSVã‚’å‡ºåŠ›ã§ãã¾ã›ã‚“")
            return ""
        
        # output/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs('output', exist_ok=True)
        
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'output/monitor_summary_{timestamp}.csv'
        elif not output_file.startswith('output/'):
            output_file = f'output/{output_file}'
        
        # ç›£è¦–å¯¾è±¡åˆ¥ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        summary_rows = []
        
        for monitor_name in df['monitor_name'].unique():
            monitor_data = df[df['monitor_name'] == monitor_name]
            
            # ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾—
            load_time_data = monitor_data[monitor_data['metric_name'] == 'builtin:synthetic.browser.actionDuration.load']
            availability_data = monitor_data[monitor_data['metric_name'] == 'builtin:synthetic.browser.availability']
            lcp_data = monitor_data[monitor_data['metric_name'] == 'builtin:synthetic.browser.largestContentfulPaint.load']
            cls_data = monitor_data[monitor_data['metric_name'] == 'builtin:synthetic.browser.cumulativeLayoutShift.load']
            
            # å¹³å‡ãƒ­ãƒ¼ãƒ‰æ™‚é–“
            avg_load_time = load_time_data['value'].mean() if len(load_time_data) > 0 else 0
            
            # å¯ç”¨æ€§
            avg_availability = availability_data['value'].mean() if len(availability_data) > 0 else 0
            
            # Web Vitals ã‚¹ã‚³ã‚¢è¨ˆç®—
            web_vitals_score = 'Unknown'
            if len(lcp_data) > 0 and len(cls_data) > 0:
                lcp_good = len(lcp_data[lcp_data['performance_status'] == 'Good']) / len(lcp_data)
                cls_good = len(cls_data[cls_data['performance_status'] == 'Good']) / len(cls_data)
                avg_vitals = (lcp_good + cls_good) / 2
                
                if avg_vitals >= 0.8:
                    web_vitals_score = 'Good'
                elif avg_vitals >= 0.5:
                    web_vitals_score = 'Warning'
                else:
                    web_vitals_score = 'Critical'
            
            # å…¨ä½“çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰
            all_perf = monitor_data['performance_status'].value_counts()
            total_graded = all_perf.get('Good', 0) + all_perf.get('Warning', 0) + all_perf.get('Critical', 0)
            
            if total_graded > 0:
                good_ratio = all_perf.get('Good', 0) / total_graded
                critical_ratio = all_perf.get('Critical', 0) / total_graded
                
                if good_ratio >= 0.8:
                    overall_grade = 'A'
                elif good_ratio >= 0.6:
                    overall_grade = 'B'
                elif critical_ratio <= 0.1:
                    overall_grade = 'C'
                else:
                    overall_grade = 'D'
            else:
                overall_grade = 'Unknown'
            
            # ä¸»è¦ãªå•é¡Œã®ç‰¹å®š
            critical_metrics = monitor_data[monitor_data['performance_status'] == 'Critical']['metric_name'].unique()
            if len(critical_metrics) > 0:
                top_issue = f"Critical: {critical_metrics[0].replace('builtin:synthetic.browser.', '').replace('.load', '')}"
            else:
                warning_metrics = monitor_data[monitor_data['performance_status'] == 'Warning']['metric_name'].unique()
                if len(warning_metrics) > 0:
                    top_issue = f"Warning: {warning_metrics[0].replace('builtin:synthetic.browser.', '').replace('.load', '')}"
                else:
                    top_issue = 'None'
            
            summary_rows.append({
                'Monitor': monitor_name,
                'Total_Measurements': len(monitor_data),
                'Avg_Load_Time_ms': round(avg_load_time, 2),
                'Availability_Percent': round(avg_availability, 2),
                'Web_Vitals_Score': web_vitals_score,
                'Overall_Grade': overall_grade,
                'Top_Issue': top_issue,
                'Good_Count': all_perf.get('Good', 0),
                'Warning_Count': all_perf.get('Warning', 0),
                'Critical_Count': all_perf.get('Critical', 0)
            })
        
        # DataFrameã«å¤‰æ›ã—ã¦CSVå‡ºåŠ›
        summary_df = pd.DataFrame(summary_rows)
        summary_df.to_csv(output_file, index=False, encoding='utf-8')
        
        self.logger.info(f"ç›£è¦–å¯¾è±¡åˆ¥ã‚µãƒãƒªãƒ¼CSVå‡ºåŠ›å®Œäº†: {output_file}")
        return output_file

    def export_monitor_analysis_json(self, analysis: Dict, output_file: Optional[str] = None) -> str:
        """ç›£è¦–å¯¾è±¡åˆ¥åˆ†æã‚’JSONã§å‡ºåŠ›ï¼ˆæ–°æ©Ÿèƒ½ï¼‰"""
        # output/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs('output', exist_ok=True)
        
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'output/monitor_analysis_{timestamp}.json'
        elif not output_file.startswith('output/'):
            output_file = f'output/{output_file}'
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ç›£è¦–å¯¾è±¡åˆ¥åˆ†æJSONå‡ºåŠ›å®Œäº†: {output_file}")
        return output_file


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='æœ€çµ‚ç‰ˆBrowserç›£è¦–çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ v2 (ä¿®æ­£ç‰ˆ)')
    parser.add_argument('--tag', action='append', help='ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã‚¿ã‚° (ä¾‹: Owner:Koizumi)')
    parser.add_argument('--hours', type=int, default=72, help='å–å¾—æœŸé–“ï¼ˆæ™‚é–“ï¼‰')
    parser.add_argument('--resolution', default='5m', help='ãƒ‡ãƒ¼ã‚¿è§£åƒåº¦ (1h, 30m, 15m, 5m)')
    parser.add_argument('--output', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰')
    parser.add_argument('--no-geo', action='store_true', help='åœ°ç†çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é™¤å¤–')
    parser.add_argument('--raw-time', action='store_true', help='æ™‚åˆ»èª¿æ•´ã‚’ç„¡åŠ¹åŒ–ï¼ˆå®Ÿéš›ã®è¨ˆæ¸¬æ™‚åˆ»ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰')
    parser.add_argument('--monitor-analysis', action='store_true', help='ç›£è¦–å¯¾è±¡åˆ¥åˆ†æã‚’è¿½åŠ å‡ºåŠ›ï¼ˆæ–°æ©Ÿèƒ½ï¼‰')
    
    args = parser.parse_args()
    
    # ãƒ­ã‚°è¨­å®š
    logger = setup_logging()
    logger.info("æœ€çµ‚ç‰ˆBrowserç›£è¦–çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ v2 (ä¿®æ­£ç‰ˆ) é–‹å§‹")
    
    # ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
    exporter = UltimateBrowserExporterV2("", "")
    exporter.load_env_from_file()
    
    api_token = os.getenv('DT_API_TOKEN')
    env_url = os.getenv('DT_ENV_URL')
    
    if not api_token or not env_url:
        logger.error("ç’°å¢ƒå¤‰æ•° DT_API_TOKEN, DT_ENV_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return 1
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
    exporter = UltimateBrowserExporterV2(api_token, env_url)
    
    try:
        # raw_timeã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã«è¨­å®š
        exporter.raw_time = args.raw_time
        
        # ãƒ‡ãƒ¼ã‚¿åé›†
        logger.info(f"åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹: ã‚¿ã‚°={args.tag}, æœŸé–“={args.hours}æ™‚é–“")
        df = exporter.collect_comprehensive_data(
            tags=args.tag,
            hours=args.hours,
            resolution=args.resolution,
            include_geo=not args.no_geo
        )
        
        if df.empty:
            logger.warning("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return 1
        
        # å‡ºåŠ›
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_name = args.output or f'ultimate_browser_results_v2_{timestamp}'
        
        # CSVå‡ºåŠ›ï¼ˆæ—¢å­˜æ©Ÿèƒ½ï¼‰
        csv_file = exporter.export_to_csv(df, f'{base_name}.csv')
        print(f"âœ… CSVå‡ºåŠ›: {csv_file}")
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»å‡ºåŠ›ï¼ˆæ—¢å­˜æ©Ÿèƒ½ï¼‰
        summary = exporter.generate_comprehensive_summary(df)
        summary_file = exporter.export_summary_report(summary, f'{base_name}_summary.json')
        print(f"âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ: {summary_file}")
        
        # ç›£è¦–å¯¾è±¡åˆ¥åˆ†æï¼ˆæ–°æ©Ÿèƒ½ï¼‰
        if args.monitor_analysis:
            print(f"\nğŸ” ç›£è¦–å¯¾è±¡åˆ¥åˆ†æã‚’å®Ÿè¡Œä¸­...")
            
            # ç›£è¦–å¯¾è±¡åˆ¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            monitor_analysis = exporter.generate_monitor_analysis(df)
            
            # JSONå‡ºåŠ›
            monitor_json_file = exporter.export_monitor_analysis_json(monitor_analysis, f'{base_name}_monitor_analysis.json')
            print(f"âœ… ç›£è¦–å¯¾è±¡åˆ¥åˆ†æJSON: {monitor_json_file}")
            
            # è©³ç´°CSVå‡ºåŠ›
            monitor_csv_file = exporter.export_monitor_analysis_csv(df, f'{base_name}_monitor_analysis.csv')
            print(f"âœ… ç›£è¦–å¯¾è±¡åˆ¥åˆ†æCSV: {monitor_csv_file}")
            
            # ã‚µãƒãƒªãƒ¼CSVå‡ºåŠ›
            monitor_summary_file = exporter.export_monitor_summary_csv(df, f'{base_name}_monitor_summary.csv')
            print(f"âœ… ç›£è¦–å¯¾è±¡åˆ¥ã‚µãƒãƒªãƒ¼CSV: {monitor_summary_file}")
            
            # ç›£è¦–å¯¾è±¡åˆ¥çµ±è¨ˆè¡¨ç¤º
            print(f"\nğŸ“Š ç›£è¦–å¯¾è±¡åˆ¥çµ±è¨ˆ:")
            for monitor_name in df['monitor_name'].unique():
                monitor_data = df[df['monitor_name'] == monitor_name]
                print(f"   {monitor_name}: {len(monitor_data):,} ãƒ¬ã‚³ãƒ¼ãƒ‰, {monitor_data['metric_name'].nunique()} ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
        else:
            print(f"\nğŸ’¡ ç›£è¦–å¯¾è±¡åˆ¥åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ --monitor-analysis ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤º
        print(f"\nğŸ“Š æœ€çµ‚ç‰ˆBrowserç›£è¦–çµæœã‚µãƒãƒªãƒ¼:")
        print(f"   ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df):,}")
        print(f"   ç›£è¦–æ•°: {df['monitor_name'].nunique()}")
        print(f"   ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°: {df['metric_name'].nunique()}")
        print(f"   æœŸé–“: {df['timestamp'].min()} ï½ {df['timestamp'].max()}")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        if 'metric_category' in df.columns:
            print(f"\nğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ:")
            category_stats = df.groupby('metric_category').size().sort_values(ascending=False)
            for category, count in category_stats.items():
                print(f"   {category}: {count:,} ãƒ¬ã‚³ãƒ¼ãƒ‰")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çŠ¶æ³
        if 'performance_status' in df.columns:
            perf_counts = df['performance_status'].value_counts()
            print(f"\nğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çŠ¶æ³:")
            for status, count in perf_counts.items():
                print(f"   {status}: {count:,} ãƒ¬ã‚³ãƒ¼ãƒ‰")
        
        logger.info("å‡¦ç†å®Œäº†")
        return 0
        
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main()) 